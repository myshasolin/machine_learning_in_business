{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1* Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (взяв список новостей пользователя)\n",
        "    - подсказка 1: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал\n",
        "    - подсказка 2: нужен именно idf, как вес.\n"
      ],
      "metadata": {
        "id": "S_M-VfYWt0yg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____\n",
        "_____"
      ],
      "metadata": {
        "id": "iusYrglqt6wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "импортируем библиотеки:"
      ],
      "metadata": {
        "id": "O2DH5tsm6jze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install razdel pymorphy2"
      ],
      "metadata": {
        "id": "T1fLic_tij1i"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import (f1_score, roc_auc_score, precision_score,\n",
        "                             classification_report, precision_recall_curve, \n",
        "                             confusion_matrix)\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from gensim.models import LdaModel\n",
        "from gensim.test.utils import datapath\n",
        "\n",
        "from razdel import tokenize \n",
        "import pymorphy2 \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "nltk.download('stopwords')\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "C2Cqv4-1ijyz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4dd1aa0-8a07-4e8a-ca89-a77e0dccce1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "качаем файлы:"
      ],
      "metadata": {
        "id": "4cCzFw8MViHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR\" -O articles.csv && rm -rf /tmp/cookies.txt \n",
        "!wget 'https://drive.google.com/uc?export=download&id=1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a' -O users_articles.csv\n",
        "!wget 'https://drive.google.com/uc?export=download&id=1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK' -O users_churn.csv\n",
        "!wget 'https://drive.google.com/uc?export=download&id=17wVn5YPpMjHToctGgff_KfSeWcIIlf7c' -O stopwords.txt"
      ],
      "metadata": {
        "id": "jPGL6R1Cij4A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4b4aafe-3b27-458d-cc0e-5ee71da8c081"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 22:51:54--  https://docs.google.com/uc?export=download&confirm=t&id=1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.204.100, 172.217.204.113, 172.217.204.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.204.100|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mt6d08pa8q6ao9515ribq6chjflv8q2g/1679439075000/14904333240138417226/*/1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR?e=download&uuid=4f319657-3d5e-49f4-bd48-53f92ab11183 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-21 22:51:54--  https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/mt6d08pa8q6ao9515ribq6chjflv8q2g/1679439075000/14904333240138417226/*/1zrdSHN2tq_Hj3YdbwlM3jk87Oct42XpR?e=download&uuid=4f319657-3d5e-49f4-bd48-53f92ab11183\n",
            "Resolving doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)... 173.194.214.132, 2607:f8b0:400c:c0b::84\n",
            "Connecting to doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)|173.194.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 145159860 (138M) [text/csv]\n",
            "Saving to: ‘articles.csv’\n",
            "\n",
            "articles.csv        100%[===================>] 138.43M   111MB/s    in 1.2s    \n",
            "\n",
            "2023-03-21 22:51:55 (111 MB/s) - ‘articles.csv’ saved [145159860/145159860]\n",
            "\n",
            "--2023-03-21 22:51:56--  https://drive.google.com/uc?export=download&id=1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.217.139, 173.194.217.100, 173.194.217.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.217.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c3nlr39p60oaa17lp04usnakl2or8621/1679439075000/14904333240138417226/*/1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a?e=download&uuid=f787ab8d-3d30-4dd9-9c27-991234d4af15 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-21 22:51:56--  https://doc-04-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/c3nlr39p60oaa17lp04usnakl2or8621/1679439075000/14904333240138417226/*/1Q97K9eGrvpbS4ut5CphZa--gJDRqQp2a?e=download&uuid=f787ab8d-3d30-4dd9-9c27-991234d4af15\n",
            "Resolving doc-04-c0-docs.googleusercontent.com (doc-04-c0-docs.googleusercontent.com)... 173.194.214.132, 2607:f8b0:400c:c0b::84\n",
            "Connecting to doc-04-c0-docs.googleusercontent.com (doc-04-c0-docs.googleusercontent.com)|173.194.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 434166 (424K) [text/csv]\n",
            "Saving to: ‘users_articles.csv’\n",
            "\n",
            "users_articles.csv  100%[===================>] 423.99K  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-03-21 22:51:56 (48.2 MB/s) - ‘users_articles.csv’ saved [434166/434166]\n",
            "\n",
            "--2023-03-21 22:51:56--  https://drive.google.com/uc?export=download&id=1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.217.139, 173.194.217.100, 173.194.217.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.217.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rmu5rlso4f3kfsi0plmgs6ri5a18johh/1679439075000/14904333240138417226/*/1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK?e=download&uuid=a7275ad0-1932-4005-b1ed-2070e7936244 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-21 22:51:57--  https://doc-0k-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/rmu5rlso4f3kfsi0plmgs6ri5a18johh/1679439075000/14904333240138417226/*/1nWKteQgEr9Rl8CwTRY7N2f7igevNH7oK?e=download&uuid=a7275ad0-1932-4005-b1ed-2070e7936244\n",
            "Resolving doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)... 173.194.214.132, 2607:f8b0:400c:c0b::84\n",
            "Connecting to doc-0k-c0-docs.googleusercontent.com (doc-0k-c0-docs.googleusercontent.com)|173.194.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 80010 (78K) [text/csv]\n",
            "Saving to: ‘users_churn.csv’\n",
            "\n",
            "users_churn.csv     100%[===================>]  78.13K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2023-03-21 22:51:57 (63.6 MB/s) - ‘users_churn.csv’ saved [80010/80010]\n",
            "\n",
            "--2023-03-21 22:51:57--  https://drive.google.com/uc?export=download&id=17wVn5YPpMjHToctGgff_KfSeWcIIlf7c\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.217.139, 173.194.217.100, 173.194.217.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.217.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/oglbmgghm523gut2h3ng1rgu307krk9c/1679439075000/14904333240138417226/*/17wVn5YPpMjHToctGgff_KfSeWcIIlf7c?e=download&uuid=dccdffe1-4ed2-4548-884d-582ce4e64b1e [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-21 22:51:57--  https://doc-0s-c0-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/oglbmgghm523gut2h3ng1rgu307krk9c/1679439075000/14904333240138417226/*/17wVn5YPpMjHToctGgff_KfSeWcIIlf7c?e=download&uuid=dccdffe1-4ed2-4548-884d-582ce4e64b1e\n",
            "Resolving doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)... 173.194.214.132, 2607:f8b0:400c:c0b::84\n",
            "Connecting to doc-0s-c0-docs.googleusercontent.com (doc-0s-c0-docs.googleusercontent.com)|173.194.214.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5368 (5.2K) [text/plain]\n",
            "Saving to: ‘stopwords.txt’\n",
            "\n",
            "stopwords.txt       100%[===================>]   5.24K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-03-21 22:51:57 (54.5 MB/s) - ‘stopwords.txt’ saved [5368/5368]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ниже код:"
      ],
      "metadata": {
        "id": "PpKq62cyVqp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_processing(df):\n",
        "    '''обработка текста'''\n",
        "\n",
        "    def clean_text(text):\n",
        "        '''очистка текста, на выходе очищеный текст'''\n",
        "        \n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        text = text.lower()\n",
        "        text = text.strip('\\n').strip('\\r').strip('\\t')\n",
        "        text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
        "        text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
        "        text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
        "        text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
        "        text = re.sub('n', ' ', text)\n",
        "        return text\n",
        "\n",
        "    cache = {}\n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "    def lemmatization(text):    \n",
        "        '''лемматизация, на выходе лист лемматизированых токенов'''\n",
        "\n",
        "        # w - стоп-слова\n",
        "        w = stopwords.words('russian')\n",
        "        with open('stopwords.txt') as f:\n",
        "            additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
        "        w += additional_stopwords \n",
        "        \n",
        "        # если зашел тип не `str` делаем его `str`\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        # токенизация предложения через razdel\n",
        "        tokens = list(tokenize(text))\n",
        "        words = [_.text for _ in tokens]\n",
        "\n",
        "        words_lem = []\n",
        "        for w in words:\n",
        "            if w[0] == '-': # проверка есть ли в начале слова '-'\n",
        "                w = w[1:]\n",
        "            if len(w) > 1: # проверка токена с одного символа\n",
        "                if w in cache: # проверка есть ли данное слово в кэше\n",
        "                    words_lem.append(cache[w])\n",
        "                else: # лемматизация слова\n",
        "                    temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
        "                    words_lem.append(temp_cach)\n",
        "        # проверка на стоп-слова:\n",
        "        words_lem_without_stopwords = [i for i in words_lem if not i in w] \n",
        "        return words_lem_without_stopwords\n",
        "    df['title'] = df['title'].progress_apply(lambda x: clean_text(x))\n",
        "    df['title'] = df['title'].progress_apply(lambda x: lemmatization(x))\n",
        "\n",
        "    return df['title']"
      ],
      "metadata": {
        "id": "VFkPQJJmij6w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trained_model(df, N_topic=20, passes=2):\n",
        "    '''создание модели'''\n",
        "    \n",
        "    # сформируем список наших текстов\n",
        "    texts = list(news['title'].values)\n",
        "    # Создадим корпус из списка с текстами\n",
        "    common_dictionary = Dictionary(texts)\n",
        "    common_corpus = [common_dictionary.doc2bow(text) for text in texts]\n",
        "    # Обучаем модель на корпусе\n",
        "    lda = LdaModel(common_corpus, num_topics=N_topic, \n",
        "                   id2word=common_dictionary, passes=passes)\n",
        "    # Сохраняем модель на диск\n",
        "    temp_file = datapath(\"model.lda\")\n",
        "    lda.save(temp_file)\n",
        "    # Загружаем обученную модель с диска\n",
        "    return common_dictionary, LdaModel.load(temp_file)"
      ],
      "metadata": {
        "id": "5bSsW36lij9Y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def documents_to_vector(df, lda, common_dictionary, N_topic=15):\n",
        "    '''функция возвращает датафрейм из вероятностей новостей'''\n",
        "\n",
        "    def get_lda_vector(lda, text):\n",
        "        '''функция возвращает векторное представление новости'''\n",
        "        \n",
        "        unseen_doc = common_dictionary.doc2bow(text)\n",
        "        lda_tuple = lda[unseen_doc]\n",
        "\n",
        "        not_null_topics = dict(zip([i[0] for i in lda_tuple], \n",
        "                                  [i[1] for i in lda_tuple]))\n",
        "        output_vector = []\n",
        "        for i in range(N_topic):\n",
        "            if i not in not_null_topics:\n",
        "                output_vector.append(0)\n",
        "            else:\n",
        "                output_vector.append(not_null_topics[i])\n",
        "        return np.array(output_vector)\n",
        "\n",
        "    topic_matrix = pd.DataFrame([get_lda_vector(lda, text) for text in df['title'].values])\n",
        "    topic_matrix.columns = [f'topic_{i}' for i in range(N_topic)]\n",
        "    topic_matrix['doc_id'] = df['doc_id'].values\n",
        "    topic_matrix = topic_matrix[['doc_id']+[f'topic_{i}' for i in range(N_topic)]]\n",
        "    return topic_matrix\n"
      ],
      "metadata": {
        "id": "UdibxRwLXnFf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_best_scores():\n",
        "    '''функция возвращает метрики, взвешивая новости по tfidf, работает с параметрами глобальных переменных'''\n",
        "\n",
        "    def get_user_embedding_idf(user_news_list, doc_dict, func=np.median):\n",
        "        \n",
        "        user_news_list = eval(user_news_list)\n",
        "        user_vector = np.zeros((len(user_news_list), N_topic))\n",
        "        for i, doc_id in enumerate(user_news_list):\n",
        "            # на случай, если новости не окажется\n",
        "            try:\n",
        "                weight = idf[idf['article'] == str(doc_id)]['idf'].values[0]\n",
        "            except Exception as e:\n",
        "                weight = 0\n",
        "            # документ умножаем на его вес\n",
        "            user_vector[i] = doc_dict[doc_id] * weight\n",
        "        user_vector = func(user_vector, axis=0)\n",
        "        return user_vector\n",
        "\n",
        "    doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[[f'topic_{i}' for i in range(N_topic)]].values))\n",
        "    # готовим строку - номер статьи будет выступать как слово\n",
        "    users['articles_string'] = users['articles'].apply(lambda x: x.replace('[','').replace(']', '').replace(',', ''))\n",
        "    # обучаемся на модели и забираем только idf, так как нам важен именно обратный вес по документу\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf.fit(users['articles_string'])\n",
        "    idf = pd.DataFrame({'article': tfidf.get_feature_names_out(), 'idf': tfidf.idf_})\n",
        "\n",
        "    user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_idf(x, doc_dict))])\n",
        "    user_embeddings.columns = [f'topic_{i}' for i in range(N_topic)]\n",
        "    user_embeddings['uid'] = users['uid'].values\n",
        "    user_embeddings = user_embeddings[['uid']+[f'topic_{i}' for i in range(N_topic)]]\n",
        "\n",
        "    X = pd.merge(user_embeddings, target, 'left')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X[[f'topic_{i}' for i in range(N_topic)]], X['churn'], random_state=0)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
        "    fscore = (2 * precision * recall) / (precision + recall)\n",
        "    ix = np.argmax(fscore)\n",
        "    roc_auc = roc_auc_score(y_test, preds)\n",
        "\n",
        "    return thresholds[ix], fscore[ix], precision[ix], recall[ix], roc_auc"
      ],
      "metadata": {
        "id": "EVirenOB2eU4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "загружаем файлы:"
      ],
      "metadata": {
        "id": "dA-hmA9HWT0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news = pd.read_csv(\"articles.csv\")\n",
        "users = pd.read_csv(\"users_articles.csv\")\n",
        "target = pd.read_csv(\"users_churn.csv\")"
      ],
      "metadata": {
        "id": "2re_k7BHWFnu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "подготавливаем текст:"
      ],
      "metadata": {
        "id": "wSLzYUUHWa9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news['title'] = word_processing(news)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6OYGvrkWYev",
        "outputId": "abea78d6-4d6c-4fb4-a996-248aa3e9af6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 27000/27000 [00:49<00:00, 542.57it/s] \n",
            "100%|██████████| 27000/27000 [03:12<00:00, 139.93it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "создаём LDA-модель (passes - количество проходов):"
      ],
      "metadata": {
        "id": "EgjGsFiofHzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_topic = 20\n",
        "common_dictionary, lda = trained_model(news, N_topic=N_topic, passes=5)\n",
        "\n",
        "# формируем матрицу вероятностей для новостей\n",
        "topic_matrix = documents_to_vector(news, lda, common_dictionary, N_topic=N_topic)"
      ],
      "metadata": {
        "id": "Ha8o1yyDWFlC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thresholds, fscore, precision, recall, roc_auc = get_best_scores()"
      ],
      "metadata": {
        "id": "sg3AS0Xw4NwA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{thresholds=:.2f}\\n{fscore=:.2f}\\n{precision=:.2f}\\n{recall=:.2f}\\n{roc_auc=:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOCH7OKs4Nya",
        "outputId": "6bb48f6a-806c-44bd-997f-1275ad9c60cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thresholds=0.35\n",
            "fscore=0.87\n",
            "precision=0.83\n",
            "recall=0.91\n",
            "roc_auc=0.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____\n",
        "_____"
      ],
      "metadata": {
        "id": "tvGQ6gloeFba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сформировать на выходе единую таблицу, сравнивающую качество 2/3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
        "### Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
      ],
      "metadata": {
        "id": "XeJzJZHYd4kM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "|  | thresholds | F-Score | precision | recall | ROC-auc |\n",
        "|-------------|-------------|-------------|-------------|-------------|-------------|\n",
        "| mean  | 0.4  | 0.74  | 0.71  | 0.78  | 0.97  |\n",
        "| median  | 0.47  | 0.83  | 0.87  | 0.84  | 0.98  |\n",
        "| max  | 0.41  | 0.79  | 0.82  | 0.81  | 0.97  |\n",
        "| idf median  | 0.35  | 0.87  | 0.83  | 0.91  | 0.99  |\n"
      ],
      "metadata": {
        "id": "yC0QKA_uD-mX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вывод**: в работе исследовались разные гиперпараметры для модели:\n",
        "- количеств тем от 5-ти до 20-ти\n",
        "- количесвтво проходов от 1 до 5-ти\n",
        "- виды агрегации: среднее, медиана, максимум\n",
        "\n",
        "Практически всегда учшие показатели модель даёт при 20-ти темах и 5-ти проходах (только при максимуме лучшим результат был при 3-х проходах)\n",
        "\n",
        "При 5-ти проходах для любой агрегации метрика модель даёт хороший результат, но медиана показала себя лучше всех, так как она менее чувствительна к выбросам.\n",
        "\n",
        "А алгоритм со взвешенными документами и медианой показал себя лучше всех, так как не все новости одинаково важны, а он добавляет вес уникальным. Аналогичные \"другим\" показатели этот алгоритм даёт уже при 10-ти новостях, а при 20-ти опережает \"других\" по всем метрикам, это видно в сводной таблице"
      ],
      "metadata": {
        "id": "-MPcmDglO_ci"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftlqhvYrTwpU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}